{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul  4 17:30:03 2017\n",
    "\n",
    "@author: vignesh\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Limits\n",
    "MINP = 1e-11\n",
    "MAXP = 1 - MINP\n",
    "MAXEXP = 700\n",
    "MINEXP = -MAXEXP\n",
    "       \n",
    "\n",
    "    \n",
    "        \n",
    "class Cascade:    \n",
    "    def populate_cascade_weights(self, share_weights):        \n",
    "        # Binary search routine\n",
    "        def binsearch(lis, key):\n",
    "            l = 0\n",
    "            r = len(lis) - 1\n",
    "            while(l <= r):\n",
    "                mid = int((l + r) / 2)\n",
    "                if lis[mid].f_id == key:\n",
    "                    return True\n",
    "                elif lis[mid].f_id > key:\n",
    "                    r = mid - 1\n",
    "                else:\n",
    "                    l = mid + 1\n",
    "            return False                        \n",
    "        n_stages = len(self.stages)\n",
    "        weights = []\n",
    "        # Populate biases for each stage\n",
    "        for i in range(n_stages):\n",
    "            temp = np.zeros(n_stages)\n",
    "            temp[i] = 1\n",
    "            w = Weight(temp, -1, 0.01, 10)\n",
    "            weights.append(w)        \n",
    "        # Weight sharing among different stages\n",
    "        if share_weights:\n",
    "            for i in range(self.n_features):\n",
    "                temp = np.zeros(n_stages)\n",
    "                for j in range(n_stages):\n",
    "                    if binsearch(self.stages[j].features, i):\n",
    "                        temp[j] = 1\n",
    "                w = Weight(temp, i, 0.01, 10)\n",
    "                weights.append(w)         \n",
    "        # No weight sharing among different stages\n",
    "        else:\n",
    "            for i in range(self.n_features):\n",
    "                for j in range(n_stages):\n",
    "                    if binsearch(self.stages[j].features, i):\n",
    "                        temp = np.zeros(n_stages)\n",
    "                        temp[j] = 1\n",
    "                        w = Weight(temp, i, 0.01, 10)\n",
    "                        weights.append(w)\n",
    "        return weights\n",
    "    \n",
    "    def update_stage_weights(self):\n",
    "        n_stages = len(self.stages)\n",
    "        \n",
    "        # Reset the stage's weights\n",
    "        for i in range(n_stages):\n",
    "            self.stages[i].weights.clear()\n",
    "        \n",
    "        # Populate it again\n",
    "        for weight in self.weights:\n",
    "            for i in range(n_stages):\n",
    "                if weight.stage_hash[i]:\n",
    "                    self.stages[i].weights.append(weight.val)\n",
    "                    \n",
    "    def reset_all_weights(self):\n",
    "        n_weights = len(self.weights)  \n",
    "        # Reset cascade weights\n",
    "        for i in range(n_weights):\n",
    "            self.weights[i].val = 0.01 \n",
    "        # Reset stage weights\n",
    "        self.update_stage_weights()  \n",
    "        \n",
    "    def reset_all_thresholds(self):\n",
    "        n_stages = len(self.stages)\n",
    "        for i in range(n_stages):\n",
    "            self.stages[i].threshold = 0\n",
    "        self.thresholds.clear()\n",
    "    \n",
    "    def __init__(self, stages, n_features, share_weights):\n",
    "        self.stages = stages\n",
    "        # Number of features\n",
    "        self.n_features = n_features\n",
    "        # Populate weights of the cascade\n",
    "        self.weights = self.populate_cascade_weights(share_weights)\n",
    "        # Populate weights[] array of each stage in the cascade\n",
    "        self.update_stage_weights()\n",
    "        # Stores final thresholds\n",
    "        self.thresholds = []    \n",
    "        # Populate cost of each stage in the cascade\n",
    "        n_stages = len(stages)\n",
    "        extracted = False * np.ones(self.n_features)\n",
    "        for i in range(n_stages):\n",
    "            for feature in self.stages[i].features:\n",
    "                if not extracted[feature.f_id]:\n",
    "                    self.stages[i].cost += feature.cost\n",
    "                    extracted[feature.f_id] = True\n",
    "                    \n",
    "    def classify(self, x):\n",
    "        cost = 0\n",
    "        n_stages = len(self.stages)\n",
    "        for i in range(n_stages - 1):\n",
    "            # Initialiize empty subset of features for the given stage\n",
    "            subset_x = [1]\n",
    "            # Append the features corresponding to the given stage from the feature vector\n",
    "            for feature in self.stages[i].features:\n",
    "                subset_x.append(x[feature.f_id])\n",
    "            cost += self.stages[i].cost        \n",
    "            if self.stages[i].pass_probability(subset_x) < self.stages[i].threshold:\n",
    "                if self.stages[i].probability(subset_x) >= 0.5:\n",
    "                    return 1, cost, i \n",
    "                else:\n",
    "                    return 0, cost, i     \n",
    "        # Handle last stage separately\n",
    "        subset_x = [1]\n",
    "        for feature in self.stages[n_stages - 1].features:\n",
    "            subset_x.append(x[feature.f_id])\n",
    "        cost += self.stages[n_stages - 1].cost\n",
    "        if self.stages[n_stages - 1].probability(subset_x) >= 0.5:\n",
    "            return 1, cost, n_stages - 1\n",
    "        else:\n",
    "            return 0, cost, n_stages - 1\n",
    "    \n",
    "    def compute_accuracy(self, X, Y, print_category):\n",
    "            acc = 0\n",
    "            acquisition_cost = 0\n",
    "            total_stages_cost = 0\n",
    "            n_examples = len(X) \n",
    "            count_correct = []\n",
    "            count_wrong = []\n",
    "            for stage in self.stages:\n",
    "                total_stages_cost += stage.cost\n",
    "            for i in range(n_examples):\n",
    "                category, cost, stage_no = self.classify(X[i])\n",
    "                if print_category:\n",
    "                    print(category)\n",
    "                acquisition_cost += cost\n",
    "                if category == Y[i]:\n",
    "                    acc += 1\n",
    "                    count_correct.append(stage_no + 1)\n",
    "                else:\n",
    "                    count_wrong.append(stage_no + 1)\n",
    "            return 100 * acc / n_examples, acquisition_cost / (n_examples * total_stages_cost), count_correct, count_wrong\n",
    "    \n",
    "    def train(self, X, Y, low_ALPHA, high_ALPHA, step_ALPHA, BETA, ETA, EPSILON, ITERATIONS, DEC_PERIOD, DEC_FACTOR, low_THRESH, high_THRESH, step_THRESH, visualize, stats):        \n",
    "        n_stages = len(self.stages)\n",
    "        n_weights = len(self.weights)   \n",
    "\n",
    "        #======================================================#\n",
    "        # ALPHA : l1-norm coefficient                          #\n",
    "        # low_ALPHA : lower limit of ALPHA for tuning          #\n",
    "        # high_ALPHA : upper limit of ALPHA for tuning         #\n",
    "        # step_ALPHA : step size for searching ALPHA           #\n",
    "        # BETA : mean cost coefficient                         #\n",
    "        # ETA : initial learning rate                          # \n",
    "        # EPSILON : convergence parameter                      #\n",
    "        # ITERATIONS : maximum number of iteraions             #\n",
    "        # DEC_PERIOD : period for reducing the learning rate   #\n",
    "        # DEC_FACTOR : reduce factor for learning rate         #\n",
    "        # low_THRESH : lower limit for threshold selection     #\n",
    "        # high_THRESH : upper limit for threshold selection    #\n",
    "        # step_THRESH : step size for threshold selection      #\n",
    "        #======================================================#    \n",
    "        \n",
    "        def precompute_subsets(X):\n",
    "            subset = []\n",
    "            for x in X:\n",
    "                temp1 = []\n",
    "                for stage in self.stages:\n",
    "                    temp2 = [1]\n",
    "                    for feature in stage.features:\n",
    "                        temp2.append(x[feature.f_id])\n",
    "                    temp1.append(temp2)\n",
    "                subset.append(temp1)\n",
    "            return np.array(subset)\n",
    "        \n",
    "        def compute_positive_prob(subset):\n",
    "            prob = []\n",
    "            n = len(subset)\n",
    "            for i in range(n):\n",
    "                prob.append(0)\n",
    "                prod = 1\n",
    "                for j in range(n_stages):\n",
    "                    if j > 0:\n",
    "                        prev_pass_prob = self.stages[j - 1].pass_probability(subset[i][j - 1])\n",
    "                        # Overflow check maybe required\n",
    "                        prod = prod * prev_pass_prob\n",
    "                    pos_prob =  self.stages[j].probability(subset[i][j])\n",
    "                    if j == n_stages - 1:\n",
    "                        prob[i] = min(max(prob[i] + prod * pos_prob, MINP), MAXP)\n",
    "                    else:\n",
    "                        pass_prob = self.stages[j].pass_probability(subset[i][j])\n",
    "                        prob[i] = min(max(prob[i] + prod * (1 - pass_prob) * pos_prob, MINP), MAXP)\n",
    "            return np.array(prob)\n",
    "            \n",
    "        # TRAIN SUB-ROUTINE STARTS HERE\n",
    "        # =============================        \n",
    "        def train_helper(ALPHA, ETA, subset_train, Y_train):\n",
    "            convergence = False            \n",
    "            cycles = []\n",
    "            train_error = []\n",
    "            iterations = 0     \n",
    "            n_train = subset_train.shape[0]\n",
    "            # Newton's Algorithm begins\n",
    "            while not convergence and iterations <= ITERATIONS:        \n",
    "                # Adaptive learning rate\n",
    "                if iterations % DEC_PERIOD == 0 and iterations != 0:\n",
    "                    ETA /= DEC_FACTOR                \n",
    "                # Increment iterations\n",
    "                iterations += 1                \n",
    "                # Compute initial probabilities for each example at the beginning of the cycle\n",
    "                p = compute_positive_prob(subset_train)\n",
    "                sum_p = sum(p)                    \n",
    "                dp_dw = []\n",
    "                dT_dw = []\n",
    "                d2p_dw2 = [] \n",
    "                d2T_dw2 = []                \n",
    "                for j in range(n_weights):                    \n",
    "                    temp_dp_dw = []\n",
    "                    temp_d2p_dw2 = []\n",
    "                    total_sum_dT_dw = 0\n",
    "                    total_sum_d2T_dw2 = 0                    \n",
    "                    for i in range(n_train):                        \n",
    "                        # Computing first and second derivatives of the probabilities\n",
    "                        sum_dp_dw = 0\n",
    "                        sum_d2p_dw2 = 0\n",
    "                        # Prefix product of pass functions (pass probabilities)\n",
    "                        prod = [1]\n",
    "                        # First derivative of the prefix product\n",
    "                        prod_1der = [0]\n",
    "                        # Second derivative of the prefix product\n",
    "                        prod_2der = [0]                        \n",
    "                        for k in range(n_stages):                \n",
    "                            if k > 0:\n",
    "                                prev_pass_prob = self.stages[k - 1].pass_probability(subset_train[i][k - 1])\n",
    "                                prev_pass_prob_1der = self.stages[k - 1].pass_probability_1der(subset_train[i][k - 1]) * self.weights[j].stage_hash[k - 1] \n",
    "                                prev_pass_prob_2der = self.stages[k - 1].pass_probability_2der(subset_train[i][k - 1]) * self.weights[j].stage_hash[k - 1]\n",
    "                                # If the weight is not a bias\n",
    "                                if self.weights[j].f_id != -1:\n",
    "                                    prev_pass_prob_1der *= X_train[i][self.weights[j].f_id]\n",
    "                                    prev_pass_prob_2der *= X_train[i][self.weights[j].f_id] ** 2\n",
    "                                # Using Product Rule\n",
    "                                # Second derivative \n",
    "                                prod_2der.append(prod[-1] * prev_pass_prob_2der + 2 * prod_1der[-1] * prev_pass_prob_1der + prev_pass_prob * prod_2der[-1] )\n",
    "                                # First derivative \n",
    "                                prod_1der.append(prod[-1] * prev_pass_prob_1der + prev_pass_prob * prod_1der[-1])\n",
    "                                # Overflow check maybe required\n",
    "                                prod.append(prod[-1] * prev_pass_prob)\n",
    "                            pos_prob =  self.stages[k].probability(subset_train[i][k])                            \n",
    "                            if k == n_stages - 1:\n",
    "                                # Remaining term's first and second derivatives\n",
    "                                rem_1der = pos_prob * (1 - pos_prob) * self.weights[j].stage_hash[k]\n",
    "                                rem_2der = pos_prob * (1 - pos_prob) * (1 - 2 * pos_prob) * self.weights[j].stage_hash[k]\n",
    "                                # If the weight is not a bias\n",
    "                                if self.weights[j].f_id != -1:\n",
    "                                    rem_1der *= X_train[i][self.weights[j].f_id]\n",
    "                                    rem_2der *= X_train[i][self.weights[j].f_id] ** 2\n",
    "                                # Product Rule \n",
    "                                # First derivative\n",
    "                                term_dp_dw = prod[-1] * rem_1der + pos_prob * prod_1der[-1]\n",
    "                                # Second derivative\n",
    "                                term_d2p_dw2 = prod[-1] * rem_2der + 2 * prod_1der[-1] * rem_1der + pos_prob * prod_2der[-1]                            \n",
    "                            else:\n",
    "                                pass_prob = self.stages[k].pass_probability(subset_train[i][k])\n",
    "                                pass_prob_1der = self.stages[k].pass_probability_1der(subset_train[i][k])\n",
    "                                pass_prob_2der = self.stages[k].pass_probability_2der(subset_train[i][k])\n",
    "                                # Remaining term's first and second derivatives\n",
    "                                rem_1der = (1 - pos_prob) * (1 - pass_prob) - pass_prob_1der\n",
    "                                rem_2der = (1 - pos_prob) * rem_1der - ((1 - pos_prob) * pass_prob_1der + (1 - pass_prob) * pos_prob * (1 - pos_prob) + pass_prob_2der)\n",
    "                                rem_1der *= pos_prob * self.weights[j].stage_hash[k]\n",
    "                                rem_2der *= pos_prob * self.weights[j].stage_hash[k]\n",
    "                                # If the weight is not a bias\n",
    "                                if self.weights[j].f_id != -1:\n",
    "                                    rem_1der *= X_train[i][self.weights[j].f_id]\n",
    "                                    rem_2der *= X_train[i][self.weights[j].f_id] ** 2\n",
    "                                # Product Rule \n",
    "                                # First derivative\n",
    "                                term_dp_dw = prod[-1] * rem_1der + (1 - pass_prob) * pos_prob * prod_1der[-1]\n",
    "                                # Second derivative\n",
    "                                term_d2p_dw2 = prod[-1] * rem_2der + 2 * prod_1der[-1] * rem_1der + (1 - pass_prob) * pos_prob * prod_2der[-1]                             \n",
    "                            sum_dp_dw += term_dp_dw\n",
    "                            sum_d2p_dw2 += term_d2p_dw2                             \n",
    "                        temp_dp_dw.append(sum_dp_dw)\n",
    "                        temp_d2p_dw2.append(sum_d2p_dw2)                        \n",
    "                        # Computing first and second derivatives of the cost\n",
    "                        sum_dT_dw = 0\n",
    "                        sum_d2T_dw2 = 0\n",
    "                        for l in range(n_stages):    \n",
    "                            sum_dT_dw += self.stages[l].cost * prod_1der[l] \n",
    "                            sum_d2T_dw2 += self.stages[l].cost * prod_2der[l]\n",
    "                        total_sum_dT_dw += sum_dT_dw    \n",
    "                        total_sum_d2T_dw2 += sum_d2T_dw2                        \n",
    "                    dp_dw.append(temp_dp_dw)\n",
    "                    d2p_dw2.append(temp_d2p_dw2)\n",
    "                    dT_dw.append(total_sum_dT_dw)\n",
    "                    d2T_dw2.append(total_sum_d2T_dw2)                       \n",
    "                # Conversion to numpy arrays \n",
    "                p = np.array(p)\n",
    "                dp_dw = np.array(dp_dw)\n",
    "                dT_dw = np.array(dT_dw)\n",
    "                d2p_dw2 = np.array(d2p_dw2) \n",
    "                d2T_dw2 = np.array(d2T_dw2)                \n",
    "                M1 = Y_train / p \n",
    "                M2 = np.ones(n_train) - Y_train\n",
    "                M3 = np.ones(n_train) - p \n",
    "                M4 = M1 - M2 / M3      \n",
    "                dl_dw = np.dot(dp_dw, M4)                \n",
    "                # Get list of weight values\n",
    "                weights_val = []\n",
    "                for weight in self.weights:\n",
    "                    weights_val.append(weight.val)    \n",
    "                dnorm_dw = np.sign(weights_val)\n",
    "                dJ_dw = -1 * dl_dw + ALPHA * dnorm_dw + BETA * dT_dw                \n",
    "                M5 = np.dot(d2p_dw2, M4) \n",
    "                M6 = Y_train / p ** 2 + M2 / M3 ** 2\n",
    "                M7 = np.dot(dp_dw ** 2, M6)\n",
    "                d2l_dw2 = M5 - M7\n",
    "                d2J_dw2 = -1 * d2l_dw2 + BETA * d2T_dw2                \n",
    "                # Newton-Raphson update \n",
    "                dw = -1 * dJ_dw / d2J_dw2\n",
    "                # Gradient Descent update \n",
    "                # dw = -dJ_dw                \n",
    "                for i in range(n_weights):\n",
    "                    dw[i] = min(max(dw[i], -self.weights[i].trust_region), self.weights[i].trust_region)\n",
    "                    self.weights[i].trust_region = max(2 * abs(dw[i]), self.weights[i].trust_region / 2)\n",
    "                # Update self.weights\n",
    "                for i in range(n_weights):\n",
    "                    self.weights[i].val += ETA * dw[i]\n",
    "                # Update stage weights\n",
    "                self.update_stage_weights()                     \n",
    "                # Compute new +ve probability for each example\n",
    "                p_new = compute_positive_prob(subset_train)                                \n",
    "                # Compute training-loss and soft training accuracy\n",
    "                soft_acc = 0\n",
    "                sum_abs_diff = 0\n",
    "                log_likelihood = 0\n",
    "                for i in range(n_train):\n",
    "                    sum_abs_diff += abs(p_new[i] - p[i])\n",
    "                    log_likelihood += Y_train[i] * np.log(p_new[i]) + (1 - Y_train[i]) * np.log(1 - p_new[i])\n",
    "                    if p_new[i] >= 0.5:\n",
    "                        category = 1\n",
    "                    else:\n",
    "                        category = 0\n",
    "                    if category == Y_train[i]:\n",
    "                        soft_acc += 1\n",
    "                soft_acc /= n_train\n",
    "                soft_acc *= 100                \n",
    "                # Convergence criterion\n",
    "                convergence = sum_abs_diff/sum_p <= EPSILON                \n",
    "                # Compute training error (without the regularization term)\n",
    "                J_train = -log_likelihood                \n",
    "                if visualize:                                                          \n",
    "                    # Visualize the graph of  negative of penalized log-likelihood VS no. of cycles\n",
    "                    cycles.append(iterations)\n",
    "                    train_error.append(J_train) \n",
    "                    plt.plot(cycles, train_error, 'ro')\n",
    "                    plt.axis([0, 100, 0, 1000])\n",
    "                    plt.xlabel(\"Cycle number\")\n",
    "                    plt.ylabel(\"Training loss\")\n",
    "                    plt.show()                \n",
    "                if stats:\n",
    "                    # Print Statistics\n",
    "                    print(\"Epoch %d: Training loss : %f | Soft Training Accuracy : %.2f %%\" % (iterations, J_train, soft_acc))           \n",
    "                    print(\"-----------------------------------------------------------\")                                    \n",
    "            # Newton's Algorithm ends\n",
    "        # TRAIN SUB-ROUTINE ENDS HERE\n",
    "        # ===========================\n",
    "        \n",
    "        #=====================================================================================================\n",
    "        # THRESHOLD HELPER BEGINS HERE\n",
    "        #=====================================================================================================\n",
    "        # Searches for suitable thresholds through a grid search\n",
    "        def threshold_helper(cur, metric):\n",
    "            # Base case\n",
    "            if cur == n_stages - 1:\n",
    "                acc, cost, count_c, count_w = self.compute_accuracy(X_cross, Y_cross, False)\n",
    "                if (1 - acc / 100) + BETA * cost < metric:\n",
    "                    # Update metric\n",
    "                    metric = (1 - acc / 100) + BETA * cost\n",
    "                    self.thresholds.clear()\n",
    "                    for i in range(n_stages):\n",
    "                        self.thresholds.append(self.stages[i].threshold) \n",
    "                return metric\n",
    "            # Recursion \n",
    "            i = low_THRESH\n",
    "            while(i < high_THRESH):\n",
    "                self.stages[cur].threshold = i\n",
    "                metric = threshold_helper(cur + 1, metric)\n",
    "                i += step_THRESH\n",
    "            return metric\n",
    "        #=====================================================================================================\n",
    "        # THRESHOLD HELPER ENDS HERE\n",
    "        #=====================================================================================================\n",
    "        \n",
    "        # Search for suitable value of ALPHA\n",
    "        min_metric = np.inf       \n",
    "        ALPHA = low_ALPHA\n",
    "        BEST_ALPHA = 0\n",
    "        while(ALPHA < high_ALPHA):\n",
    "            seed = 11\n",
    "            kfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = seed)\n",
    "            cv_metric = []\n",
    "            for train, cross in kfold.split(X, Y):\n",
    "                X_train = X[train]\n",
    "                X_cross = X[cross]\n",
    "                Y_train = Y[train]\n",
    "                Y_cross = Y[cross]\n",
    "                subset_train = precompute_subsets(X_train)\n",
    "                train_helper(ALPHA, ETA, subset_train, Y_train)\n",
    "                metric = threshold_helper(0, np.inf)\n",
    "                self.reset_all_weights()\n",
    "                self.reset_all_thresholds()\n",
    "                cv_metric.append(metric)\n",
    "            if  np.mean(cv_metric) < min_metric:\n",
    "                # Update minimum metric\n",
    "                min_metric = np.mean(cv_metric)\n",
    "                # Update best ALPHA\n",
    "                BEST_ALPHA = ALPHA        \n",
    "            # Display cross-validation metric\n",
    "            print(\"ALPHA = %.2f | Cross-validation metric : %f +- %f\" % (ALPHA, min_metric, np.std(cv_metric)))\n",
    "            print(\"===========================================================\\n\")            \n",
    "            # Update ALPHA\n",
    "            ALPHA *= step_ALPHA            \n",
    "        subset_train = precompute_subsets(X)\n",
    "        train_helper(BEST_ALPHA, ETA, subset_train, Y)\n",
    "        threshold_helper(0, np.inf)             \n",
    "        # Set thresholds to corresponding arg-max \n",
    "        for i in range(n_stages):\n",
    "            self.stages[i].threshold = self.thresholds[i]  \n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        acc, cost, count_c, count_w = self.compute_accuracy(X, Y, False)\n",
    "        return acc, cost, count_c, count_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Feature:    \n",
    "    def __init__(self, f_id, cost, f_name):\n",
    "        # Feature index according to dataset\n",
    "        self.f_id = f_id \n",
    "        self.cost = cost\n",
    "        self.f_name = f_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Weight:    \n",
    "    def __init__(self, stage_hash, f_id, val, trust_region):\n",
    "        self.stage_hash = stage_hash\n",
    "        # If it's a bias, then f_id = -1\n",
    "        self.f_id = f_id\n",
    "        self.val = val\n",
    "        self.trust_region = trust_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stage:    \n",
    "    def __init__(self, features, f_soft_pass, f_soft_pass_1der, f_soft_pass_2der, s_name):\n",
    "        self.features =  features\n",
    "        # Sort in increasing order of id for binary search while populating cascade weights\n",
    "        self.features.sort(key = lambda x: x.f_id)\n",
    "        # Pass function\n",
    "        self.f_soft_pass = f_soft_pass\n",
    "        # Pass function first derivative\n",
    "        self.f_soft_pass_1der = f_soft_pass_1der\n",
    "        # Pass function second derivative\n",
    "        self.f_soft_pass_2der = f_soft_pass_2der\n",
    "        self.s_name = s_name\n",
    "        self.cost = 0\n",
    "        self.threshold = 0\n",
    "        self.weights = [] \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        x = min(max(x, MINEXP), MAXEXP)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # In this case, pass probability is NOT EQUAL to the sigmoid-probability for the given stage\n",
    "    # Instead, it is defined as a separate function\n",
    "    # The sigmoid-probability is used ultimately in the hard cascade as before\n",
    "    def probability(self, x):\n",
    "        return min(max(self.sigmoid(np.dot(self.weights, x)), MINP), MAXP)\n",
    "        \n",
    "    # The pass probability is used only for training the soft cascade\n",
    "    def pass_probability(self, x):\n",
    "        return min(max(self.f_soft_pass(np.dot(self.weights, x)), MINP), MAXP)\n",
    "        \n",
    "    def pass_probability_1der(self, x):\n",
    "        return self.f_soft_pass_1der(np.dot(self.weights, x))\n",
    "        \n",
    "    def pass_probability_2der(self, x):\n",
    "        return self.f_soft_pass_2der(np.dot(self.weights, x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
